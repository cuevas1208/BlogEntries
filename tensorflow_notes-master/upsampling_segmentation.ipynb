{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post, we saw how to do _Image Classification_\n",
    "by performing crop of the central part of an image and\n",
    "making an inference using one of the standart classification models.\n",
    "After that, we saw how to perform the network inference on the whole\n",
    "image by changing the network to _fully convolutional_ one. This approach\n",
    "gave us a downsampled prediction map for the image -- that happened due\n",
    "to the fact that _max-pooling_ layers are used in the network architecture.\n",
    "This prediction map can be treated as an efficient way to make an inference of\n",
    "the network for the whole image. You can also think about it as a way to make\n",
    "_Image Segmentation_, but it is not an actual _Segmentation_, because\n",
    "the standart network models were trained to perform _Classification_.\n",
    "To make it perform an actual _Segmentation_, we will have to train it\n",
    "on _Segmentation_ dataset in a special way like in the paper _Fully convolutional\n",
    "networks for semantic segmentation_ by Long et al. Two of the most popular general\n",
    "_Segmentation_ datasets are: [Microsoft COCO](http://mscoco.org/) and [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n",
    "\n",
    "In this post, we will perform image upsampling to get the prediction map\n",
    "that is of the same size as an input image. We will do this using _transposed convolution_\n",
    "(also known as _deconvolution_). It is recommended not to use the _deconvolution_ name for this\n",
    "operation as it can be confused with another operation and it does not represent accurately\n",
    "the actual process that is being performed. The most accurate name for the kind of operation\n",
    "that we will perform in this post is _fractionally strided convolution_. \n",
    "We will cover a small part of theory necessary for understanding and some resources will be cited.\n",
    "\n",
    "One question might be raised up now: Why do we need to perform upsampling using _fractionally\n",
    "strided convolution_? Why can't we just use some library to do this for us? The answer is: we\n",
    "need to do this because we need to define the upsampling operation as a layer in the network.\n",
    "And why do we need it as a layer? Because we will have to perform training where the image and\n",
    "respective _Segmentation_ groundtruth will be given to us -- and we will have to perform\n",
    "training using backpropagation.\n",
    "As it is [known](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/) , each layer in the network has to be able to perform three operations:\n",
    "_forward propagation_, _backward propagation_ and _update_ which performs updates\n",
    "to the weights of the layer during training. By doing the upsampling with _transposed convolution_\n",
    "we will have all of these operations defined and we will be able to perform training.\n",
    "\n",
    "By the end of the post, we will implement the upsampling and will make sure it is correct\n",
    "by comparing it to the implementation of the [scikit-image library](http://scikit-image.org/).\n",
    "To be more specific we will have _FCN-32_ _Segmentation_ network implemented which is\n",
    "described in the paper _Fully convolutional networks for semantic segmentation_.\n",
    "To perform the training, the loss function has to be defined and training dataset provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run the provided code, follow the previous post and run the\n",
    "following command to run the code on the first _GPU_ and specify the folder\n",
    "with downloaded classification models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "sys.path.append(\"/home/dpakhom1/workspace/models/slim\")\n",
    "\n",
    "# A place where you have downloaded a network checkpoint -- look at the previous post\n",
    "checkpoints_dir = '/home/dpakhom1/checkpoints'\n",
    "sys.path.append(\"/home/dpakhom1/workspace/models/slim\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Image Upsampling_ is a specific case of _Resampling_.\n",
    "According to a definition, provided in [this article about _Resampling_](http://avisynth.nl/index.php/Resampling):\n",
    "\n",
    "\n",
    "```\n",
    "The idea behind resampling is to reconstruct the continuous signal from the original sampled signal and resample it again using more samples (which is called interpolation or upsampling) or fewer samples (which is called decimation or downsampling)\n",
    "\n",
    "```\n",
    "\n",
    "In other words, we can approximate the continious signal from the points that\n",
    "we have and sample new ones from the reconstructed signal. So, to be more specific,\n",
    "in our case, we have downsampled prediction map -- these are points from which we\n",
    "want to reconstruct original signal. And if we are able to approximate original\n",
    "signal, we can sample more points and, therefore, perform upsampling.\n",
    "\n",
    "We say \"to approximate\" here because the continious signal will most probably\n",
    "not be reconstructed perfectly. Under a certain ideal conditions, the signal\n",
    "can be perfectly reconstructed though. There is a _Nyquist–Shannon sampling theorem_\n",
    "that states that the signal can be ideally reconstructed if _x(t) contains no\n",
    "sinusoidal component at exactly frequency B, or that B must be strictly less than one half\n",
    "the sample rate_. Basically, the sampling frequency should be bigger by the factor of two\n",
    "than the biggest frequency that input signal contains.\n",
    "\n",
    "But what is the exact equation to get this reconstruction? Taking the equation from this [source](http://avisynth.nl/index.php/Resampling):\n",
    "\n",
    "\n",
    "s(x) = sum_n s(n*T) * sinc((x-n*T)/T), with sinc(x) = sin(pi*x)/(pi*x) for x!=0, and = 1 for x=0\n",
    "\n",
    "with fs = 1/T the sampling rate, s(n*T) the samples of s(x) and sinc(x) the resampling kernel.\n",
    "\n",
    "[Wikipedia article](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) has a great explanation of the equation:\n",
    "\n",
    "```\n",
    "A mathematically ideal way to interpolate the sequence involves the use of sinc functions. Each sample in the sequence is replaced by a sinc function, centered on the time axis at the original location of the sample, nT, with the amplitude of the sinc function scaled to the sample value, x[n]. Subsequently, the sinc functions are summed into a continuous function. A mathematically equivalent method is to convolve one sinc function with a series of Dirac delta pulses, weighted by the sample values. Neither method is numerically practical. Instead, some type of approximation of the sinc functions, finite in length, is used. The imperfections attributable to the approximation are known as interpolation error.\n",
    "```\n",
    "\n",
    "So, we can see that the continious signal is reconstructed by placing the resampling kernel\n",
    "function at each point that we have and summing up everything (this explanation omits some \n",
    "details for simplicity). It should be stated here that the resampling kernel shouldn't necessary\n",
    "be sinc function. For example, the bilinear resampling kernel can be useв. You can find more\n",
    "examples [here](http://avisynth.nl/index.php/Resampling). Also, one important point\n",
    "from the explanation above is that mathematically equivalent way is to convolve the\n",
    "kernel function with series of Dirac delta pulses, weighted by the sample values. These\n",
    "two equavalent ways to perform reconstruction are important as they will make understanding\n",
    "of how transposed convolution work and that each transposed convolution has an equivalent\n",
    "convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform image upsampling using built-in function from [scikit-image library](http://scikit-image.org/).\n",
    "We will need this to validate if our implementation of bilinear upsampling is correct later in the post.\n",
    "This is exactly how the implementation of bilinear upsampling [was validated](http://nbviewer.jupyter.org/gist/tnarihi/54744612d35776f53278) before [being merged](https://github.com/BVLC/caffe/pull/2213). Part of the code in the post was taken from\n",
    "there. Below we will perform the upsampling with _factor_ 3 meaning that an output size\n",
    "will be three times bigger than an input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cueva\\Anaconda3\\envs\\nightly\\lib\\site-packages\\skimage\\io\\_plugins\\matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n",
      "C:\\Users\\cueva\\Anaconda3\\envs\\nightly\\lib\\site-packages\\matplotlib\\axes\\_base.py:1400: MatplotlibDeprecationWarning: The 'box-forced' keyword argument is deprecated since 2.2.\n",
      "  \" since 2.2.\", cbook.mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x185a427a358>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import ogrid, repeat, newaxis\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# Generate image that will be used for test upsampling\n",
    "# Number of channels is 3 -- we also treat the number of\n",
    "# samples like the number of classes, because later on\n",
    "# that will be used to upsample predictions from the network\n",
    "imsize = 3\n",
    "x, y = ogrid[:imsize, :imsize]\n",
    "img = repeat((x + y)[..., newaxis], 3, 2) / float(imsize + imsize)\n",
    "io.imshow(img, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cueva\\Anaconda3\\envs\\nightly\\lib\\site-packages\\skimage\\io\\_plugins\\matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n",
      "C:\\Users\\cueva\\Anaconda3\\envs\\nightly\\lib\\site-packages\\matplotlib\\axes\\_base.py:1400: MatplotlibDeprecationWarning: The 'box-forced' keyword argument is deprecated since 2.2.\n",
      "  \" since 2.2.\", cbook.mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x185a4d06a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage.transform\n",
    "\n",
    "def upsample_skimage(factor, input_img):\n",
    "    \n",
    "    # Pad with 0 values, similar to how Tensorflow does it.\n",
    "    # Order=1 is bilinear upsampling\n",
    "    return skimage.transform.rescale(input_img,\n",
    "                                     factor,\n",
    "                                     mode='constant',\n",
    "                                     cval=0,\n",
    "                                     order=1)\n",
    "\n",
    "\n",
    "upsampled_img_skimage = upsample_skimage(factor=3, input_img=img)\n",
    "io.imshow(upsampled_img_skimage, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Transposed convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper by Long et al. it was stated that upsampling can be performed using\n",
    "fractionally strided convolution (transposed convolution). But first it is\n",
    "necessary to understand how transposed convolution works.To understand that,\n",
    "we should look at a usual convolution and see that it convolves\n",
    "the image and depending on the parameters (stride, kernel size, padding) reduces\n",
    "the input image. What if we would be able to perform an operation that goes\n",
    "in the opposite direction -- from small input to the bigger one while preserving the\n",
    "connectivity pattern. Here is an illustration:\n",
    "\n",
    "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/Deconv_exp.PNG)\n",
    "\n",
    "Convolution is a linear operation and, therefore, it can be represented as a matrix\n",
    "multiplication. To achieve the result described above, we only need to traspose the\n",
    "matrix that defines a particular convolution. The resulted operation is no longer\n",
    "a convolution, but it can still be represented as a convolution, which won't be \n",
    "as efficient as transposing a convolution. To get more information about the equivalence\n",
    "and transposed convolution in general we refer reader to [this paper](https://arxiv.org/pdf/1609.07009.pdf)\n",
    "and [this guide](https://arxiv.org/pdf/1603.07285.pdf).\n",
    "\n",
    "So, if we define a bilinear upsampling kernel and perform fractionally strided\n",
    "convolution on the image, we will get an upsampled output, which will be defined\n",
    "as a layer in the network and will make it possible for us to perform backpropagation.\n",
    "For the FCN-32 we will use bilinear upsampling kernel as an initialization, meaning that\n",
    "the network can learn a more suitable kernel during backpropagation.\n",
    "\n",
    "To make the code below more easy to read, we will provide some statements that can be\n",
    "derived from the [following article](http://avisynth.nl/index.php/Resampling). The _factor_\n",
    "of upsampling is equal to the stride of transposed convolution. The kernel of the upsampling\n",
    "operation is determined by the identity: __2 * factor - factor % 2__.\n",
    "\n",
    "Below, we will define the bilinear interpolation using transposed convolution operation\n",
    "in Tensorflow. We will perform this operation on cpu, because later in the post we will\n",
    "need the same piece of code to perfom memory consuming operation that won't fit into GPU.\n",
    "After performing the interpolation, we compare our results to the results that were\n",
    "obtained by the function from scikit-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-20aeca5380bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mupsampled_img_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupsample_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupsampled_img_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-20aeca5380bb>\u001b[0m in \u001b[0;36mupsample_tf\u001b[1;34m(factor, input_img)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mlogits_pl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mupsample_filter_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbilinear_upsample_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 res = tf.nn.conv2d_transpose(logits_pl, upsample_filt_pl,\n",
      "\u001b[1;32m<ipython-input-4-20aeca5380bb>\u001b[0m in \u001b[0;36mbilinear_upsample_weights\u001b[1;34m(factor, number_of_classes)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mupsample_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupsample_filt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupsample_kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_kernel_size(factor):\n",
    "    \"\"\"\n",
    "    Find the kernel size given the desired factor of upsampling.\n",
    "    \"\"\"\n",
    "    return 2 * factor - factor % 2\n",
    "\n",
    "\n",
    "def upsample_filt(size):\n",
    "    \"\"\"\n",
    "    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "    \"\"\"\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "\n",
    "def bilinear_upsample_weights(factor, number_of_classes):\n",
    "    \"\"\"\n",
    "    Create weights matrix for transposed convolution with bilinear filter\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    filter_size = get_kernel_size(factor)\n",
    "    \n",
    "    weights = np.zeros((filter_size,\n",
    "                        filter_size,\n",
    "                        number_of_classes,\n",
    "                        number_of_classes), dtype=np.float32)\n",
    "    \n",
    "    upsample_kernel = upsample_filt(filter_size)\n",
    "    \n",
    "    for i in xrange(number_of_classes):\n",
    "        \n",
    "        weights[:, :, i, i] = upsample_kernel\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "def upsample_tf(factor, input_img):\n",
    "    \n",
    "    number_of_classes = input_img.shape[2]\n",
    "    \n",
    "    new_height = input_img.shape[0] * factor\n",
    "    new_width = input_img.shape[1] * factor\n",
    "    \n",
    "    expanded_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "\n",
    "                upsample_filt_pl = tf.placeholder(tf.float32)\n",
    "                logits_pl = tf.placeholder(tf.float32)\n",
    "\n",
    "                upsample_filter_np = bilinear_upsample_weights(factor, number_of_classes)\n",
    "\n",
    "                res = tf.nn.conv2d_transpose(logits_pl, upsample_filt_pl,\n",
    "                                 output_shape=[1, new_height, new_width, number_of_classes],\n",
    "                                 strides=[1, factor, factor, 1])\n",
    "\n",
    "                final_result = sess.run(res, feed_dict={upsample_filt_pl: upsample_filter_np,\n",
    "                                             logits_pl: expanded_img})\n",
    "    \n",
    "    return final_result.squeeze()\n",
    "\n",
    "upsampled_img_tf = upsample_tf(factor=3, input_img=img)\n",
    "io.imshow(upsampled_img_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the results of upsampling are the same\n",
    "np.allclose(upsampled_img_skimage, upsampled_img_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in xrange(2, 10):\n",
    "    \n",
    "    upsampled_img_skimage = upsample_skimage(factor=factor, input_img=img)\n",
    "    upsampled_img_tf = upsample_tf(factor=factor, input_img=img)\n",
    "    \n",
    "    are_equal = np.allclose(upsampled_img_skimage, upsampled_img_tf)\n",
    "    \n",
    "    print(\"Check for factor {}: {}\".format(factor, are_equal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampled predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's apply our upsampling to the actual predictions. We will take the _VGG-16_\n",
    "model that we used in the previous post for classification and apply our upsampling\n",
    "to the downsampled predictions that we get from the network.\n",
    "\n",
    "Before applying the code below I had to change a [certain line](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/contrib/slim/python/slim/nets/vgg.py#L165)\n",
    "in the definition of _VGG-16_ model to prevent it from reducing the size even more. To be more specific,\n",
    "I had to change the _7x7_ convolutional layer padding to _SAME_ option. This was done by the authors\n",
    "because they wanted to get single prediction for the input image of standart size. But in case of segmentation\n",
    "we don't need this, because otherwise by upsampling by factor 32 we won't get\n",
    "the image of the same size as the input. After making the aforementioned change, the issue was eliminated.\n",
    "\n",
    "Be careful because the code below and specifically the upsampling variable consumes a huge\n",
    "amount of space (~15 Gb). This is due to the fact that we have huge filters _64 by 64_\n",
    "and _1000_ classes. Moreover, we actually don't use a lot of space of the upsampling variable,\n",
    "because we define only the diagonal submatrices, therefore, a lot of space is wasted.\n",
    "This is only done for demonstration purposes for _1000_ classes and standart _Segmentation_\n",
    "datasets usually contain less classes (20 classes on PASCAL VOC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib2\n",
    "\n",
    "from datasets import imagenet\n",
    "from nets import vgg\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "checkpoints_dir = '/home/dpakhom1/checkpoints'\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Load the mean pixel values and the function\n",
    "# that performs the subtraction\n",
    "from preprocessing.vgg_preprocessing import (_mean_image_subtraction,\n",
    "                                            _R_MEAN, _G_MEAN, _B_MEAN)\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "# Function to nicely print segmentation results with\n",
    "# colorbar showing class names\n",
    "def discrete_matshow(data, labels_names=[], title=\"\"):\n",
    "    \n",
    "    fig_size = [7, 6]\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    \n",
    "    #get discrete colormap\n",
    "    cmap = plt.get_cmap('Paired', np.max(data)-np.min(data)+1)\n",
    "    \n",
    "    # set limits .5 outside true range\n",
    "    mat = plt.matshow(data,\n",
    "                      cmap=cmap,\n",
    "                      vmin = np.min(data)-.5,\n",
    "                      vmax = np.max(data)+.5)\n",
    "    #tell the colorbar to tick at integers\n",
    "    cax = plt.colorbar(mat,\n",
    "                       ticks=np.arange(np.min(data),np.max(data)+1))\n",
    "    \n",
    "    # The names to be printed aside the colorbar\n",
    "    if labels_names:\n",
    "        cax.ax.set_yticklabels(labels_names)\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=15, fontweight='bold')\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    url = (\"https://upload.wikimedia.org/wikipedia/commons/d/d9/\"\n",
    "           \"First_Student_IC_school_bus_202076.jpg\")\n",
    "    \n",
    "    image_string = urllib2.urlopen(url).read()\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    # Convert image to float32 before subtracting the\n",
    "    # mean pixel value\n",
    "    image_float = tf.to_float(image, name='ToFloat')\n",
    "    \n",
    "    # Subtract the mean pixel value from each pixel\n",
    "    processed_image = _mean_image_subtraction(image_float,\n",
    "                                              [_R_MEAN, _G_MEAN, _B_MEAN])\n",
    "\n",
    "    input_image = tf.expand_dims(processed_image, 0)\n",
    "    \n",
    "    with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "        \n",
    "        # spatial_squeeze option enables to use network in a fully\n",
    "        # convolutional manner\n",
    "        logits, _ = vgg.vgg_16(input_image,\n",
    "                               num_classes=1000,\n",
    "                               is_training=False,\n",
    "                               spatial_squeeze=False)\n",
    "    \n",
    "    # For each pixel we get predictions for each class\n",
    "    # out of 1000. We need to pick the one with the highest\n",
    "    # probability. To be more precise, these are not probabilities,\n",
    "    # because we didn't apply softmax. But if we pick a class\n",
    "    # with the highest value it will be equivalent to picking\n",
    "    # the highest value after applying softmax\n",
    "    pred = tf.argmax(logits, dimension=3)\n",
    "    \n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'vgg_16.ckpt'),\n",
    "        slim.get_model_variables('vgg_16'))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_fn(sess)\n",
    "        segmentation, np_image, np_logits = sess.run([pred, image, logits])\n",
    "\n",
    "# Remove the first empty dimension\n",
    "segmentation = np.squeeze(segmentation)\n",
    "\n",
    "names = imagenet.create_readable_names_for_imagenet_labels()\n",
    "\n",
    "# Let's get unique predicted classes (from 0 to 1000) and\n",
    "# relable the original predictions so that classes are\n",
    "# numerated starting from zero\n",
    "unique_classes, relabeled_image = np.unique(segmentation,\n",
    "                                            return_inverse=True)\n",
    "\n",
    "segmentation_size = segmentation.shape\n",
    "\n",
    "relabeled_image = relabeled_image.reshape(segmentation_size)\n",
    "\n",
    "labels_names = []\n",
    "\n",
    "for index, current_class_number in enumerate(unique_classes):\n",
    "\n",
    "    labels_names.append(str(index) + ' ' + names[current_class_number+1])\n",
    "\n",
    "# Show the downloaded image\n",
    "plt.figure()\n",
    "plt.imshow(np_image.astype(np.uint8))\n",
    "plt.suptitle(\"Input Image\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "discrete_matshow(data=relabeled_image, labels_names=labels_names, title=\"Segmentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's upsample the predictions that we got for the image using the bilinear\n",
    "upsampling kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = upsample_tf(factor=32, input_img=np_logits.squeeze())\n",
    "upsampled_predictions = upsampled_logits.squeeze().argmax(axis=2)\n",
    "\n",
    "unique_classes, relabeled_image = np.unique(upsampled_predictions,\n",
    "                                            return_inverse=True)\n",
    "\n",
    "relabeled_image = relabeled_image.reshape(upsampled_predictions.shape)\n",
    "\n",
    "labels_names = []\n",
    "\n",
    "for index, current_class_number in enumerate(unique_classes):\n",
    "\n",
    "    labels_names.append(str(index) + ' ' + names[current_class_number+1])\n",
    "\n",
    "# Show the downloaded image\n",
    "plt.figure()\n",
    "plt.imshow(np_image.astype(np.uint8))\n",
    "plt.suptitle(\"Input Image\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "discrete_matshow(data=relabeled_image, labels_names=labels_names, title=\"Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput results that we got are quite noisy, but we got an approximate segmentation for the bus.\n",
    "To be more precise, it is not a segmentation but regions where the network was evaluated\n",
    "and gave the following predictions.\n",
    "\n",
    "The next stage is to perform training of the whole system on a specific Segmentation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we covered the transposed convolution and specifically the\n",
    "implementation of bilinear interpolation using transposed convolution. We applied\n",
    "it to downsampled predictions to upsample them and get predictions for the whole\n",
    "input image.\n",
    "\n",
    "We also saw that you might experience problems with space if you do segmentation\n",
    "for a huge number of classes. Because of that, we had to perform that operation\n",
    "on CPU and load weights of upsampling filter into RAM instead of GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nightly]",
   "language": "python",
   "name": "conda-env-nightly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
